{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9afab6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from sklearn.metrics import (classification_report, precision_recall_curve, \n",
    "                            roc_auc_score, average_precision_score, confusion_matrix,\n",
    "                            precision_score, recall_score, f1_score)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Fix pandas pickle compatibility\n",
    "if 'pandas.core.indexes.numeric' not in sys.modules:\n",
    "    import pandas.core.indexes.api as idx_api\n",
    "    sys.modules['pandas.core.indexes.numeric'] = idx_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86a8b88",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# DATA LOADING AND FEATURE ENGINEERING\n",
    "\n",
    "class FraudDataProcessor:\n",
    "    \"\"\"Handles loading, cleaning, and feature engineering\"\"\"\n",
    "    \n",
    "    def __init__(self, zip_path='dataset.zip'):\n",
    "        self.zip_path = zip_path\n",
    "        self.df = None\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"Load all daily pickle files from zip\"\"\"\n",
    "        print(\"Loading data from zip file...\")\n",
    "        all_dfs = []\n",
    "        \n",
    "        with zipfile.ZipFile(self.zip_path, 'r') as zip_ref:\n",
    "            pickle_files = sorted([f for f in zip_ref.namelist() if f.endswith('.pkl')])\n",
    "            \n",
    "            for i, pkl_file in enumerate(pickle_files):\n",
    "                try:\n",
    "                    with zip_ref.open(pkl_file) as f:\n",
    "                        # Use pd.read_pickle with compatibility mode\n",
    "                        df_day = pd.read_pickle(f)\n",
    "                        all_dfs.append(df_day)\n",
    "                except Exception as e:\n",
    "                    # Fallback: extract and load with pickle module\n",
    "                    print(f\"Using fallback loader for {pkl_file}\")\n",
    "                    with zip_ref.open(pkl_file) as f:\n",
    "                        import sys\n",
    "                        import io\n",
    "                        # Monkey patch for compatibility\n",
    "                        if 'pandas.core.indexes.numeric' not in sys.modules:\n",
    "                            import pandas.core.indexes.api as idx_api\n",
    "                            sys.modules['pandas.core.indexes.numeric'] = idx_api\n",
    "                        df_day = pickle.load(f)\n",
    "                        all_dfs.append(df_day)\n",
    "                    \n",
    "                if (i + 1) % 20 == 0:\n",
    "                    print(f\"Loaded {i + 1}/{len(pickle_files)} files\")\n",
    "        \n",
    "        self.df = pd.concat(all_dfs, ignore_index=True)\n",
    "        print(f\"Total transactions loaded: {len(self.df):,}\")\n",
    "        print(f\"Fraud rate: {self.df['TX_FRAUD'].mean()*100:.4f}%\")\n",
    "        return self\n",
    "    \n",
    "    def engineer_features(self):\n",
    "        \"\"\"Create time-based and aggregation features (optimized)\"\"\"\n",
    "        print(\"\\nEngineering features...\")\n",
    "        df = self.df.copy()\n",
    "        \n",
    "        # Convert datetime\n",
    "        df['TX_DATETIME'] = pd.to_datetime(df['TX_DATETIME'])\n",
    "        df = df.sort_values(['CUSTOMER_ID', 'TX_DATETIME']).reset_index(drop=True)\n",
    "        \n",
    "        # Basic datetime features\n",
    "        df['hour'] = df['TX_DATETIME'].dt.hour\n",
    "        df['weekday'] = df['TX_DATETIME'].dt.weekday\n",
    "        df['day'] = df['TX_DATETIME'].dt.day\n",
    "        df['is_weekend'] = (df['weekday'] >= 5).astype(int)\n",
    "        df['is_night'] = ((df['hour'] >= 22) | (df['hour'] <= 6)).astype(int)\n",
    "        \n",
    "        # Amount features\n",
    "        df['amount_log'] = np.log1p(df['TX_AMOUNT'])\n",
    "        df['amount_gt_220'] = (df['TX_AMOUNT'] > 220).astype(int)\n",
    "        \n",
    "        # Efficient rolling features using expanding window approach\n",
    "        print(\"Computing customer rolling features...\")\n",
    "        \n",
    "        # For each window, compute expanding statistics then shift\n",
    "        for window_days in [1, 7, 14, 28]:\n",
    "            window_name = f'{window_days}d'\n",
    "            print(f\"  Processing {window_name} window...\")\n",
    "            \n",
    "            # Customer transaction count\n",
    "            df[f'cust_tx_count_{window_name}'] = (\n",
    "                df.groupby('CUSTOMER_ID').cumcount() + 1\n",
    "            )\n",
    "            \n",
    "            # Customer amount statistics (expanding)\n",
    "            df[f'cust_tx_amt_mean_{window_name}'] = (\n",
    "                df.groupby('CUSTOMER_ID')['TX_AMOUNT']\n",
    "                .expanding().mean()\n",
    "                .reset_index(level=0, drop=True)\n",
    "            )\n",
    "            \n",
    "            df[f'cust_tx_amt_std_{window_name}'] = (\n",
    "                df.groupby('CUSTOMER_ID')['TX_AMOUNT']\n",
    "                .expanding().std()\n",
    "                .reset_index(level=0, drop=True)\n",
    "            ).fillna(0)\n",
    "            \n",
    "            # Shift by 1 to avoid leakage (don't include current transaction)\n",
    "            for col in [f'cust_tx_count_{window_name}', \n",
    "                       f'cust_tx_amt_mean_{window_name}',\n",
    "                       f'cust_tx_amt_std_{window_name}']:\n",
    "                df[col] = df.groupby('CUSTOMER_ID')[col].shift(1).fillna(0)\n",
    "        \n",
    "        # Sort by terminal for terminal features\n",
    "        df = df.sort_values(['TERMINAL_ID', 'TX_DATETIME']).reset_index(drop=True)\n",
    "        \n",
    "        # Terminal rolling features\n",
    "        print(\"Computing terminal rolling features...\")\n",
    "        for window_days in [7, 28]:\n",
    "            window_name = f'{window_days}d'\n",
    "            print(f\"  Processing terminal {window_name} window...\")\n",
    "            \n",
    "            df[f'terminal_tx_count_{window_name}'] = (\n",
    "                df.groupby('TERMINAL_ID').cumcount() + 1\n",
    "            )\n",
    "            \n",
    "            # Shift to avoid leakage\n",
    "            df[f'terminal_tx_count_{window_name}'] = (\n",
    "                df.groupby('TERMINAL_ID')[f'terminal_tx_count_{window_name}']\n",
    "                .shift(1).fillna(0)\n",
    "            )\n",
    "        \n",
    "        # Resort by datetime for proper ordering\n",
    "        df = df.sort_values('TX_DATETIME').reset_index(drop=True)\n",
    "        \n",
    "        # Derived features\n",
    "        df['amount_to_cust_mean_ratio_14d'] = (\n",
    "            df['TX_AMOUNT'] / (df['cust_tx_amt_mean_14d'] + 1e-5)\n",
    "        )\n",
    "        \n",
    "        df['amount_to_cust_std_ratio_14d'] = (\n",
    "            df['TX_AMOUNT'] / (df['cust_tx_amt_std_14d'] + 1e-5)\n",
    "        )\n",
    "        \n",
    "        # Replace inf values\n",
    "        df = df.replace([np.inf, -np.inf], 0)\n",
    "        df = df.fillna(0)\n",
    "        \n",
    "        self.df = df\n",
    "        print(\"Feature engineering complete!\")\n",
    "        print(f\"Total features: {len(df.columns)}\")\n",
    "        return self\n",
    "    \n",
    "    def split_data(self, train_ratio=0.7, val_ratio=0.15):\n",
    "        \"\"\"Time-based split (critical for fraud detection)\"\"\"\n",
    "        df = self.df.sort_values('TX_DATETIME')\n",
    "        \n",
    "        n = len(df)\n",
    "        train_idx = int(n * train_ratio)\n",
    "        val_idx = int(n * (train_ratio + val_ratio))\n",
    "        \n",
    "        train_df = df.iloc[:train_idx]\n",
    "        val_df = df.iloc[train_idx:val_idx]\n",
    "        test_df = df.iloc[val_idx:]\n",
    "        \n",
    "        print(f\"\\nData split:\")\n",
    "        print(f\"Train: {len(train_df):,} ({train_df['TX_FRAUD'].sum():,} frauds)\")\n",
    "        print(f\"Val:   {len(val_df):,} ({val_df['TX_FRAUD'].sum():,} frauds)\")\n",
    "        print(f\"Test:  {len(test_df):,} ({test_df['TX_FRAUD'].sum():,} frauds)\")\n",
    "        \n",
    "        return train_df, val_df, test_df\n",
    "    \n",
    "    def prepare_features(self, df, fit_scaler=False):\n",
    "        \"\"\"Extract and normalize features\"\"\"\n",
    "        feature_cols = [\n",
    "            'TX_AMOUNT', 'amount_log', 'amount_gt_220',\n",
    "            'hour', 'weekday', 'day', 'is_weekend', 'is_night',\n",
    "            'cust_tx_count_1d', 'cust_tx_count_7d', 'cust_tx_count_14d', 'cust_tx_count_28d',\n",
    "            'cust_tx_amt_mean_1d', 'cust_tx_amt_mean_7d', 'cust_tx_amt_mean_14d', 'cust_tx_amt_mean_28d',\n",
    "            'cust_tx_amt_std_1d', 'cust_tx_amt_std_7d', 'cust_tx_amt_std_14d', 'cust_tx_amt_std_28d',\n",
    "            'terminal_tx_count_7d', 'terminal_tx_count_28d',\n",
    "            'amount_to_cust_mean_ratio_14d', 'amount_to_cust_std_ratio_14d'\n",
    "        ]\n",
    "        \n",
    "        X = df[feature_cols].values\n",
    "        y = df['TX_FRAUD'].values\n",
    "        \n",
    "        if fit_scaler:\n",
    "            X = self.scaler.fit_transform(X)\n",
    "        else:\n",
    "            X = self.scaler.transform(X)\n",
    "        \n",
    "        return X, y, feature_cols\n",
    "\n",
    "\n",
    "\n",
    "#PYTORCH DATASET AND MODEL\n",
    "class FraudDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for fraud detection\"\"\"\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "class FraudDetectionNet(nn.Module):\n",
    "    \"\"\"Neural Network for fraud detection with residual connections\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dims=[256, 128, 64], dropout=0.3):\n",
    "        super(FraudDetectionNet, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(*layers)\n",
    "        self.classifier = nn.Linear(prev_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        output = self.classifier(features)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "#TRAINING UTILITIES\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss for handling class imbalance\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = nn.BCEWithLogitsLoss(reduction='none')(inputs, targets)\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for X_batch, y_batch in dataloader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch).squeeze()\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    \"\"\"Evaluate model\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch).squeeze()\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            \n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_labels.extend(y_batch.numpy())\n",
    "    \n",
    "    all_probs = np.array(all_probs)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # Find optimal threshold\n",
    "    precision, recall, thresholds = precision_recall_curve(all_labels, all_probs)\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "    best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "    \n",
    "    all_preds = (all_probs >= best_threshold).astype(int)\n",
    "    \n",
    "    return all_preds, all_probs, all_labels, best_threshold\n",
    "\n",
    "\n",
    "\n",
    "#MAIN PIPELINE\n",
    "def main():\n",
    "    # Set random seeds\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load and process data\n",
    "    processor = FraudDataProcessor('dataset.zip')\n",
    "    processor.load_data().engineer_features()\n",
    "    \n",
    "    train_df, val_df, test_df = processor.split_data()\n",
    "    \n",
    "    # Prepare features\n",
    "    X_train, y_train, feature_names = processor.prepare_features(train_df, fit_scaler=True)\n",
    "    X_val, y_val, _ = processor.prepare_features(val_df, fit_scaler=False)\n",
    "    X_test, y_test, _ = processor.prepare_features(test_df, fit_scaler=False)\n",
    "    \n",
    "    print(f\"\\nFeature dimension: {X_train.shape[1]}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = FraudDataset(X_train, y_train)\n",
    "    val_dataset = FraudDataset(X_val, y_val)\n",
    "    test_dataset = FraudDataset(X_test, y_test)\n",
    "    \n",
    "    # Handle class imbalance with weighted sampling\n",
    "    class_counts = np.bincount(y_train.astype(int))\n",
    "    class_weights = 1. / class_counts\n",
    "    sample_weights = class_weights[y_train.astype(int)]\n",
    "    sampler = WeightedRandomSampler(sample_weights, len(sample_weights))\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, sampler=sampler)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1024, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = FraudDetectionNet(\n",
    "        input_dim=X_train.shape[1],\n",
    "        hidden_dims=[256, 128, 64],\n",
    "        dropout=0.3\n",
    "    ).to(device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = FocalLoss(alpha=0.25, gamma=2.0)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', \n",
    "                                                      factor=0.5, patience=3)\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"\\nStarting training...\")\n",
    "    best_f1 = 0\n",
    "    patience_counter = 0\n",
    "    max_patience = 10\n",
    "    \n",
    "    for epoch in range(50):\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        \n",
    "        # Validation\n",
    "        val_preds, val_probs, val_labels, threshold = evaluate(model, val_loader, device)\n",
    "        \n",
    "        val_precision = precision_score(val_labels, val_preds)\n",
    "        val_recall = recall_score(val_labels, val_preds)\n",
    "        val_f1 = f1_score(val_labels, val_preds)\n",
    "        val_auprc = average_precision_score(val_labels, val_probs)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:02d} | Loss: {train_loss:.4f} | \"\n",
    "              f\"F1: {val_f1:.4f} | AUPRC: {val_auprc:.4f} | \"\n",
    "              f\"Threshold: {threshold:.4f}\")\n",
    "        \n",
    "        scheduler.step(val_f1)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_fraud_model.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= max_patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "    \n",
    "    # Load best model and evaluate on test set\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL EVALUATION ON TEST SET\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    model.load_state_dict(torch.load('best_fraud_model.pt'))\n",
    "    test_preds, test_probs, test_labels, test_threshold = evaluate(model, test_loader, device)\n",
    "    \n",
    "    # Metrics\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(test_labels, test_preds, target_names=['Legitimate', 'Fraud']))\n",
    "    \n",
    "    print(f\"\\nAdditional Metrics:\")\n",
    "    print(f\"ROC-AUC: {roc_auc_score(test_labels, test_probs):.4f}\")\n",
    "    print(f\"AUPRC: {average_precision_score(test_labels, test_probs):.4f}\")\n",
    "    print(f\"Optimal Threshold: {test_threshold:.4f}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(test_labels, test_preds)\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"TN: {cm[0,0]:,} | FP: {cm[0,1]:,}\")\n",
    "    print(f\"FN: {cm[1,0]:,} | TP: {cm[1,1]:,}\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix - Test Set')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"\\nConfusion matrix saved as 'confusion_matrix.png'\")\n",
    "    \n",
    "    return model, processor, feature_names\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, processor, feature_names = main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
